{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "97af0df7-0351-416f-b0c0-8a7856f9777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import PolynomialFeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "ecff2f9e-1808-405d-9432-1ce18969f225",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nyuad/anaconda3/envs/virtualenv/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    " #Activity 3. Fitlinear regression using the closed-form solution. \n",
    "\n",
    "boston = load_boston()\n",
    "\n",
    "X= boston.data\n",
    "\n",
    "Y = boston.target\n",
    "\n",
    "def determine_theta (x,y):\n",
    "    #Append a column of ones in X to add the bias term\n",
    "    x = np.c_[np.ones((x.shape[0], 1)), x]\n",
    "    n_train = x.shape[0] #Number of tarining exmaples\n",
    "    #reshaping y to (m,1)\n",
    "    y=y.reshape(n_train,1)\n",
    "    #Normal equation\n",
    "    theta = np.dot (np.linalg.inv(np.dot(x.T,x)), np.dot(x.T, y))\n",
    "    return theta\n",
    "\n",
    "def predict (x, theta) :  \n",
    "    #This could be worng as well\n",
    "    x = np.c_[np.ones((x.shape[0], 1)), x]\n",
    "    #preds is y_hat\n",
    "    preds = np.dot(x, theta)\n",
    "    return preds\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "def mean_squared_error(y_pred, y_true):\n",
    "    # Number of samples\n",
    "    n = len(y_true)\n",
    "    # Calculate the squared differences\n",
    "    squared_diffs = (y_pred - y_true) ** 2\n",
    "    # Compute the average of the squared differences (MSE)\n",
    "    mse = np.mean((squared_diffs))\n",
    "    return mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "53241d94-1812-44e5-91a0-d32fd6e56f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activity 3. Use k fold cross validation to estimate the performance of the model\n",
    "# Initialize lists to store the training and test scores\n",
    "\n",
    "def k_fold_func (x, y) :\n",
    "    kf= KFold(n_splits = 10)\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "    # Loop over each fold\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        # Split the data into training and test sets\n",
    "        X_train, X_test = x[train_index], x[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        theta = determine_theta (X_train, y_train)\n",
    "        #predictions\n",
    "        y_train_pred = predict (X_train, theta)\n",
    "        y_test_pred = predict (X_test, theta)\n",
    "        # Compute the Mean Squared Error (MSE) for both training and test sets -> Need to actually write formula\n",
    "        train_mse = mean_squared_error(y_train_pred, y_train) #mean_squared_error(y_train, y_train_pred)\n",
    "        test_mse = mean_squared_error(y_test_pred, y_test)\n",
    "        # Append the errors to the lists\n",
    "        train_errors.append(train_mse)\n",
    "        test_errors.append(test_mse)\n",
    "    # Calculate the average error across all folds\n",
    "    avg_train_error = np.mean(train_errors)\n",
    "    avg_test_error = np.mean(test_errors)\n",
    "    print(f\"Average training MSE: {avg_train_error}\")\n",
    "    print(f\"Average test MSE: {avg_test_error}\")\n",
    "\n",
    "#Need to correct this\n",
    "#k_fold_func(X, Y)\n",
    "\n",
    "#Ask about the difference between loss and error like which arguments should we input into the MSE function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "e9626901-36be-4973-bc8f-22b177714b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training MSE: 146.62656490039512\n",
      "Average test MSE: 84.68254533486471\n"
     ]
    }
   ],
   "source": [
    "#Activity 3 Print the average recorded socres for both the test and the training set\n",
    "k_fold_func(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "00c4cc2d-35fd-434a-9c26-4e4b0ccc9c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activity 4 Fit a ridge regression model using the closed solution\n",
    "\n",
    "# Use k-fold cross-validation to findthe best 位Use the Numpy function:\n",
    "#np.logspace(1, 7, num=13) to get the differentvalues for 位\n",
    "\n",
    "def Ridge_Regression_closed(x, y, lambd):\n",
    "    alpha = lambd #lambda\n",
    "    x_b = np.c_[np.ones((x.shape[0], 1)), x]\n",
    "    I = np.identity (x_b.shape[1])\n",
    "    I[0][0]=0\n",
    "    #Implementing the penalty eq\n",
    "    penalty = alpha*I    \n",
    "    #Equation to solve for b\n",
    "    b= np.linalg.inv((x_b.T @ x_b) + penalty)@(x_b.T@y)\n",
    "    return b\n",
    "\n",
    "def predict_ridge (x, theta) :  \n",
    "    #X = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "    #x = np.c_[np.ones((x.shape[0], 1)), x]\n",
    "    #preds is y_hat\n",
    "    #preds = np.dot(x, theta)\n",
    "    X_b = np.c_[np.ones((x.shape[0], 1)), x]  # Add bias term\n",
    "    return np.dot(X_b, theta)\n",
    "    #return preds\n",
    "\n",
    "#cross validation to determine the best lambda\n",
    "def ridge_cross_validation_best (x,y,op_lamb):\n",
    "    kf = KFold(n_splits=10)\n",
    "    avg_losses = [] # ask what should go into this list\n",
    "    avg_train_error = []\n",
    "    avg_test_error = []\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "    #lambda_pairs = []\n",
    "    best_mse = float('inf')\n",
    "    for iterator in op_lamb:\n",
    "        for train_index, test_index in kf.split(x):\n",
    "            X_train, X_test = x[train_index], x[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            theta = Ridge_Regression_closed(X_train, y_train, iterator)\n",
    "            #predictions\n",
    "            y_train_pred = predict_ridge (X_train, theta)\n",
    "            y_test_pred = predict_ridge (X_test, theta)\n",
    "            # Compute the Mean Squared Error (MSE) for both training and test sets -> Need to actually write formula\n",
    "            train_mse = mean_squared_error(y_train_pred, y_train)\n",
    "            test_mse = mean_squared_error(y_test_pred, y_test) #actual value loss\n",
    "            # Append the errors to the lists\n",
    "            train_errors.append(train_mse)\n",
    "            test_errors.append(test_mse)\n",
    "        # Calculate the average error across all folds\n",
    "        avg_train_error.append(np.mean(train_errors))\n",
    "        avg_test_error.append(np.mean(test_errors)) # for each one saved it corresponds to one of the op_lamb\n",
    "        #lambda_pairs.append((iterator, avg_test_error))\n",
    "        #if avg_test_error < best_mse:\n",
    "          #  best_mse = avg_test_error\n",
    "           # best_lambda = lmbda\n",
    "    #best_lambda_index = np.argmin(avg_test_error)\n",
    "    #best_lambda = op_lamb[best_lambda_index]\n",
    "      # Track the best lambda by finding the lowest test error\n",
    "        if np.mean(test_errors) < best_mse:\n",
    "            best_mse = np.mean(test_errors)\n",
    "            best_lambda = iterator\n",
    "    \n",
    "    return best_lambda\n",
    "\n",
    "#performance of ridge regression with a determined lamb\n",
    "def ridge_cross_validation (x, y, lamb) :\n",
    "    kf= KFold(n_splits = 10)\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "    # Loop over each fold\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        # Split the data into training and test sets\n",
    "        X_train, X_test = x[train_index], x[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        theta = Ridge_Regression_closed(X_train, y_train, lamb)\n",
    "        #predictions\n",
    "        y_train_pred = predict_ridge (X_train, theta)\n",
    "        y_test_pred = predict_ridge (X_test, theta)\n",
    "        # Compute the Mean Squared Error (MSE) for both training and test sets -> Need to actually write formula\n",
    "        train_mse = mean_squared_error(y_train_pred, y_train)\n",
    "        test_mse = mean_squared_error(y_test_pred, y_test)\n",
    "        # Append the errors to the lists\n",
    "        train_errors.append(train_mse)\n",
    "        test_errors.append(test_mse)\n",
    "\n",
    "    # Calculate the average error across all folds\n",
    "    avg_train_error = np.mean(train_errors)\n",
    "    avg_test_error = np.mean(test_errors)\n",
    "    print(f\"Average training MSE: {avg_train_error}\")\n",
    "    print(f\"Average test MSE: {avg_test_error}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "ead83f52-1869-449a-89a3-e49ceefeb4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best lambda value was: 316.22776601683796\n",
      "Average training MSE: 25.374775995777483\n",
      "Average test MSE: 29.799581632087218\n"
     ]
    }
   ],
   "source": [
    "#Activity 5 For the best 位 you found, use k-fold cross-validation to estimate the performance of thismodel with this 位\n",
    "# code to try functions\n",
    "op_lamb = np.logspace(1, 7, num=13)\n",
    "best_lamb = ridge_cross_validation_best (X,Y,op_lamb)\n",
    "print(f\"Best lambda value was: {best_lamb}\")\n",
    "ridge_cross_validation(X,Y,best_lamb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "1348ca21-d957-4365-b80b-6c18b1522c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activity 5. Print the average of your recorded scores for both the test set andtraining set.\n",
    "#Done above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "cb242c02-65cf-4dda-8661-6172dc3992e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best lambda value was: 10000000.0\n",
      "Average training MSE: 18.882249988235237\n",
      "Average test MSE: 34.531693305166065\n"
     ]
    }
   ],
   "source": [
    "#Activity 6. Repeat the previous exercise, but this time, by creating a polynomial transformation of degree 2 on the features of the\n",
    "#dataset.a.Using PolynomialFeatures(degree=2) in sklearn library7  \n",
    "\n",
    "poly_features = PolynomialFeatures (degree=2, include_bias =False)\n",
    "x_poly = poly_features.fit_transform (X)\n",
    "\n",
    "def Ridge_Regression_closed_poly(x, y, lambd):\n",
    "    alpha = lambd #lambda\n",
    "    x_columns1 = np.c_[np.ones((x.shape[0], 1)), x]\n",
    "    I = np.identity (x_columns1.shape[1])\n",
    "    I[0][0]=0\n",
    "    #Implementing the penalty eq\n",
    "    penalty = alpha*I    \n",
    "    #Equation to solve for b\n",
    "    b= np.linalg.inv((x_columns1.T @ x_columns1) + penalty)@(x_columns1.T@y)\n",
    "    return b\n",
    "\n",
    "def predict_ridge_poly (x, theta) :  \n",
    "    X_predictor = np.c_[np.ones((x.shape[0], 1)), x]\n",
    "    #preds is y_hat\n",
    "    preds = np.dot(X_predictor, theta)\n",
    "    return preds\n",
    "\n",
    "def ridge_cross_validation_best_poly (x,y,op_lamb):\n",
    "    kf = KFold(n_splits=10)\n",
    "    avg_losses = [] # ask what should go into this list\n",
    "    avg_train_error = []\n",
    "    avg_test_error = []\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "    #poly_param = []\n",
    "    for iterator in op_lamb:\n",
    "        for train_index, test_index in kf.split(x):\n",
    "            X_train, X_test = x[train_index], x[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            theta = Ridge_Regression_closed_poly(X_train, y_train, iterator)\n",
    "            #poly_param.append(theta)\n",
    "            #predictions\n",
    "            y_train_pred = predict_ridge_poly (X_train, theta)\n",
    "            y_test_pred = predict_ridge_poly (X_test, theta)\n",
    "            # Compute the Mean Squared Error (MSE) for both training and test sets -> Need to actually write formula\n",
    "            train_mse = mean_squared_error(y_train_pred, y_train)\n",
    "            test_mse = mean_squared_error(y_test_pred, y_test) #actual value loss\n",
    "            # Append the errors to the lists\n",
    "            train_errors.append(train_mse)\n",
    "            test_errors.append(test_mse)\n",
    "        # Calculate the average error across all folds\n",
    "        avg_train_error.append(np.mean(train_errors))\n",
    "        avg_test_error.append(np.mean(test_errors)) # for each one saved it corresponds to one of the op_lamb\n",
    "    best_lambda_index = np.argmin(avg_test_error)\n",
    "    best_lambda = op_lamb[best_lambda_index]\n",
    "    return best_lambda\n",
    "\n",
    "#performance of ridge regression with a determined lamb\n",
    "def ridge_cross_validation_poly (x, y, lamb) :\n",
    "    kf= KFold(n_splits = 10)\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "    poly_param =[]\n",
    "    # Loop over each fold\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        # Split the data into training and test sets\n",
    "        X_train, X_test = x[train_index], x[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        theta = Ridge_Regression_closed_poly(X_train, y_train, lamb)\n",
    "        poly_param.append(theta)\n",
    "        #predictions\n",
    "        y_train_pred = predict_ridge_poly (X_train, theta)\n",
    "        y_test_pred = predict_ridge_poly (X_test, theta)\n",
    "        # Compute the Mean Squared Error (MSE) for both training and test sets -> Need to actually write formula\n",
    "        train_mse = mean_squared_error(y_train_pred, y_train)\n",
    "        test_mse = mean_squared_error(y_test_pred, y_test)\n",
    "        # Append the errors to the lists\n",
    "        train_errors.append(train_mse)\n",
    "        test_errors.append(test_mse)\n",
    "    # Calculate the average error across all folds\n",
    "    avg_train_error = np.mean(train_errors)\n",
    "    avg_test_error = np.mean(test_errors)\n",
    "    print(f\"Average training MSE: {avg_train_error}\")\n",
    "    print(f\"Average test MSE: {avg_test_error}\")\n",
    "\n",
    "#Apply functions\n",
    "op_lamb_poly = np.logspace(1, 7, num=13)  # Lambda values from 10^1 to 10^7\n",
    "best_lamb_poly = ridge_cross_validation_best_poly(x_poly, Y, op_lamb_poly)\n",
    "print(f\"Best lambda value was: {best_lamb_poly}\")\n",
    "# Evaluate the performance with the best lambda\n",
    "ridge_cross_validation_poly(x_poly, Y, best_lamb_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "ea8fd283-0a14-4a60-ad37-ac4018369b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Theta: [-0.75756286  1.30948006 -0.866291   -0.76046423  1.12882979  1.11403624\n",
      "  3.18510342  0.07480554  1.093951    0.21266962 -0.02047211 -0.62668221\n",
      " -0.05048897 -0.46597186]\n",
      "Final Cost: 307.01117468232894\n"
     ]
    }
   ],
   "source": [
    "#Activity 7. Repeat Multivariate Linear Regression using the Gradient Descent method\n",
    "X_normalized = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "X_b = np.c_[np.ones((X_normalized.shape[0], 1)), X_normalized]\n",
    "\n",
    "\n",
    "def predict(X, theta):\n",
    "    return np.dot(X, theta)\n",
    "\n",
    "# Function to compute the cost (MSE)\n",
    "def compute_cost(X_c, y, theta): #x_b test\n",
    "    m = len(y)\n",
    "    y_pred = predict (X_c, theta)\n",
    "    cost = (1/(2*m)) * np.sum((y_pred - y)**2)\n",
    "    return cost\n",
    "\n",
    "# Gradient descent function\n",
    "def gradient_descent(X_c, y, theta, learning_rate, iterations):\n",
    "    m = len(y)\n",
    "    cost_history = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        y_pred = predict (X_c, theta)\n",
    "        error = y_pred - y\n",
    "        gradients = (1/m) * X_c.T.dot(error)\n",
    "        theta = theta - learning_rate * gradients\n",
    "        cost = compute_cost(X_c, y, theta)\n",
    "        cost_history.append(cost)\n",
    "    return theta, cost_history\n",
    "\n",
    "\n",
    "#Call the function\n",
    "list_thetas = []\n",
    "list_cost = []\n",
    "kf = KFold(n_splits=10)\n",
    "# Initialize hyperparameters\n",
    "iterations = 1000\n",
    "learning_rate = 0.000001\n",
    "\n",
    "\n",
    "# Train the model using KFold cross-validation\n",
    "for train_index, test_index in kf.split(X_b):\n",
    "    X_train, X_test = X_b[train_index], X_b[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "    # Initialize theta (parameter vector)\n",
    "    theta = np.random.randn(X_train.shape[1])\n",
    "    # Perform gradient descent\n",
    "    theta_optimal, cost_history = gradient_descent(X_train, y_train, theta, learning_rate, iterations)\n",
    "    list_thetas.append(theta_optimal)\n",
    "    list_cost.append(cost_history)\n",
    "    ave_cost = np.mean (cost_history)\n",
    "    # Print the final cost and optimized theta\n",
    "print(f\"Optimized Theta: {theta_optimal}\")\n",
    "print(f\"Final Cost: {ave_cost}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "984fcd48-f197-4f73-89fc-8cff19c55e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training MSE (Elastic Net): 22.675293975256317\n",
      "Average Test MSE (Elastic Net): 24.17295639525117\n"
     ]
    }
   ],
   "source": [
    "# Activity 8 Implement Lasso Regression (Text book Chapter 4)a.Cost function of Lasso Regression:\n",
    "#remember to pass x_b argument becaus ethe predict function does not adjust the matrix\n",
    "# Function to compute the cost (MSE)\n",
    "# Function to compute the cost (MSE) with L1 penalty (Lasso)\n",
    "def compute_cost_Lasso(X_c, y, theta, l1_penalty):\n",
    "    m = len(y)\n",
    "    y_pred = X_c.dot(theta)  # Prediction using the current theta\n",
    "    error = y_pred - y\n",
    "    mse = (1/(2*m)) * np.dot(error.T, error)  # Mean Squared Error (MSE)\n",
    "    l1_penalty_term = l1_penalty * np.sum(np.abs(theta[1:]))  # Exclude the intercept from regularization\n",
    "    return mse + l1_penalty_term\n",
    "\n",
    "# Gradient descent function for Lasso regression\n",
    "def gradient_descent(X_c, y, theta, learning_rate, iterations, l1_penalty):\n",
    "    m = len(y)\n",
    "    cost_history = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # Predict y using current theta\n",
    "        y_pred = X_c.dot(theta)\n",
    "        # Compute error\n",
    "        error = y_pred - y\n",
    "        # Compute gradient (without L1 regularization for intercept)\n",
    "        gradient = (1/m) * X_c.T.dot(error)\n",
    "        # Update theta (Apply L1 regularization using soft thresholding)\n",
    "        theta = theta - learning_rate * gradient\n",
    "        theta[1:] = np.sign(theta[1:]) * np.maximum(np.abs(theta[1:]) - learning_rate * l1_penalty, 0) \n",
    "        cost = compute_cost_Lasso(X_c, y, theta, l1_penalty)\n",
    "        cost_history.append(cost)\n",
    "        avg_cost = np.mean(cost_history)\n",
    "        # Compute average theta (excluding the bias term if necessary)\n",
    "        avg_theta = np.mean(theta)\n",
    "        \n",
    "    #print(avg_cost, avg_theta)\n",
    "\n",
    "    return theta, cost_history\n",
    "\n",
    "l1_penalty = 0.1\n",
    "\n",
    "#gradient_descent(X_b, Y, theta, learning_rate, iterations, l1_penalty)\n",
    "\n",
    "alpha = 0.1  \n",
    "r = 0.5      \n",
    "learning_rate = 0.01\n",
    "num_iters = 1000\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state = 42)\n",
    "ave_train = []\n",
    "ave_test = []\n",
    "\n",
    "for train_index, test_index in kf.split(X_b):\n",
    "    X_train, X_test = X_b[train_index], X_b[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    X_train = np.c_[np.ones((X_train.shape[0], 1)), X_train]\n",
    "    X_test = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "    \n",
    "    # Initialize theta with random values\n",
    "    theta = np.random.randn(X_train.shape[1])\n",
    "    \n",
    "    # Use X_train and y_train for training\n",
    "    theta, _ = gradient_descent(X_train, y_train, theta, learning_rate, iterations, l1_penalty)\n",
    "    \n",
    "    y_train_pred = predict(X_train, theta)\n",
    "    y_test_pred = predict(X_test, theta)\n",
    "    \n",
    "    train_mse = np.mean((y_train - y_train_pred)**2)\n",
    "    test_mse = np.mean((y_test - y_test_pred)**2)\n",
    "    \n",
    "    ave_train.append(train_mse)\n",
    "    ave_test.append(test_mse)\n",
    "\n",
    "print(f\"Average Training MSE (Elastic Net): {np.mean(ave_train)}\")\n",
    "print(f\"Average Test MSE (Elastic Net): {np.mean(ave_test)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "1be82cc0-0ca5-42f5-80fb-b530af020787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training MSE (Elastic Net): 493.7420009955441\n",
      "Average Test MSE (Elastic Net): 509.277903374346\n"
     ]
    }
   ],
   "source": [
    "#Activity 9 Implement Elastic Net (Text book Chapter 4) a.Cost Function of Elastic Net:\n",
    "\n",
    "def cost_elastic_net(X_c, y, theta, alpha, r):\n",
    "    m = len(y)\n",
    "    predictions = predict(X_c, theta)\n",
    "    error = (1/(2*m)) * np.sum((predictions - y)**2)\n",
    "    penalty1 = r * alpha * np.sum(np.abs(theta))\n",
    "    penalty2 = (1 - r) / 2 * alpha * np.sum(theta**2)\n",
    "    return error + penalty1 + penalty2\n",
    "\n",
    "def elastic_gradient_descent(X_c, y, theta, alpha, r, learning_rate, num_iters):\n",
    "    m = len(y)\n",
    "    cost_history = []\n",
    "    for _ in range(num_iters):\n",
    "        predictions = predict(X_c, theta)\n",
    "        # Gradient of MSE (standard gradient descent)\n",
    "        gradient_mse = (1/m) * X_c.T.dot(predictions - y)\n",
    "        # L1 penalty gradient (Lasso part)\n",
    "        gradient_l1 = r * alpha * np.sign(theta)\n",
    "        # L2 penalty gradient (Ridge part)\n",
    "        gradient_l2 = (1 - r) * alpha * theta\n",
    "        # Full gradient with both L1 and L2 regularization terms\n",
    "        gradient = gradient_mse + gradient_l1 + gradient_l2\n",
    "        # gradient descent step\n",
    "        theta = theta - learning_rate * gradient\n",
    "        cost = cost_elastic_net(X_c, y, theta, alpha, r)\n",
    "        cost_history.append(cost)\n",
    "    return theta, cost_history\n",
    "\n",
    "#remember to pass x_b argument becaus ethe predict function does not adjust the matrix\n",
    "alpha = 2\n",
    "r = 0.1      \n",
    "learning_rate = 0.000001\n",
    "num_iters = 100000\n",
    "\n",
    "kf = KFold(n_splits=10)\n",
    "ave_train = []\n",
    "ave_test = []\n",
    "\n",
    "for train_index, test_index in kf.split(X_b):\n",
    "    X_train, X_test = X_b[train_index], X_b[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "    \n",
    "    theta = np.random.randn(X_train.shape[1])\n",
    "    \n",
    "    theta1, _ = elastic_gradient_descent(X_train, y_train, theta, alpha, r, learning_rate, num_iters)\n",
    "    \n",
    "    y_train_pred = predict(X_train, theta1)\n",
    "    y_test_pred = predict(X_test, theta1)\n",
    "    \n",
    "    train_mse = np.mean((y_train - y_train_pred)**2)\n",
    "    test_mse = np.mean((y_test - y_test_pred)**2)\n",
    "    \n",
    "    ave_train.append(train_mse)\n",
    "    ave_test.append(test_mse)\n",
    "\n",
    "print(f\"Average Training MSE (Elastic Net): {np.mean(ave_train)}\")\n",
    "print(f\"Average Test MSE (Elastic Net): {np.mean(ave_test)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "62a7b135-2651-4330-b39b-bac49433c866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activity 10 If you are given a choice of predicting future housing prices using one of the modelsyou have learned above \n",
    "#(those optimized with gradient descent), which one wouldyou choose and why? State the parameters of that model.\n",
    "\n",
    "#After analyzing the Mean Test Errors I came to the realization that the Lasso Regressation had the least error. Although this could be attributed\n",
    "#to different reasons, for the sake of simplicity, this was the main factor that I am considering to select the Lasso model to predict the housing \n",
    "#prices. The parameters are alpha = 0.1  \n",
    "#r = 0.5      \n",
    "#learning_rate = 0.01\n",
    "#num_iters = 1000\n",
    "#l1_penalty = 0.1\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
